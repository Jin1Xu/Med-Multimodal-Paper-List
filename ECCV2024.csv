发表时间,来源,论文类型,论文题目,题目翻译,谷歌引用,论文地址,代码地址,Github Star,所用数据集,数据集地址,备注
2024,Oral,多模态评测；基准,MMBENCH: Is Your Multi-Modal Model an All-around Player?,MMBENCH：你的多模态模型是全能选手吗？,1615,https://arxiv.org/pdf/2307.06281,https://github.com/open-compass/MMBench?utm_source=chatgpt.com,267,,,
2024,Multi-Modal,多模态字幕；模型增强,ShareGPT4V: Improving Large Multi-Modal Models with Better Captions,ShareGPT4V：通过更优描述提升大型多模态模型,943,https://arxiv.org/abs/2311.12793,https://github.com/ShareGPT4Omni/ShareGPT4V,243,,,
2024,Oral,视频理解；长视频,LongVLM: Efficient Long Video Understanding via Large Language Models,LongVLM：基于大语言模型的高效长视频理解,431,https://arxiv.org/pdf/2404.03384?,https://github.com/ziplab/LongVLM?utm_source=chatgpt.com,107,,,
2024,Multi-Modal,多模态理解；数学视觉,MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?,MathVerse：你的多模态大模型是否真正“看懂”了视觉数学题中的图示？,415,https://arxiv.org/abs/2403.14624,https://github.com/ZrrSkywalker/MathVerse,174,,,
2024,Foundation Model,视频理解；基础模型,InternVideo2: Scaling Foundation Models for Multimodal Video Understanding,InternVideo2：面向多模态视频理解的基础模型扩展,412,https://arxiv.org/abs/2403.15377,https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2/,2100,,,
2024,Multimodal,预训练；多模态模型,"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",MM1：多模态大语言模型预训练的方法、分析与见解,382,https://arxiv.org/abs/2403.09611,https://github.com/kyegomez/MM1,24,,,
2024,Oral,推理加速；VLM,An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models,图像在第 2 层后只值半个 token：用于大规模视觉-语言模型的即插即用推理加速,344,https://arxiv.org/pdf/2403.06764,https://github.com/pkunlp-icler/FastV?utm_source=chatgpt.com,516,,,
2024,Multimodal,模型评估；多模态,BLINK: Multimodal Large Language Models Can See but Not Perceive,BLINK：多模态大语言模型能“看见”但不能“感知”,317,https://arxiv.org/abs/2404.12390,,,,,
2024,Foundation Model,物体解析；基础模型,PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects,PartGLEE：用于任意物体识别与解析的基础模型,291,https://arxiv.org/abs/2407.16696,https://github.com/ProvenceStar/PartGLEE,54,,,
2024,Multimodal,安全评测；多模态,MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models,MM-SafetyBench：多模态大语言模型安全性评测基准,217,https://arxiv.org/abs/2311.17600,https://github.com/isXinLiu/MM-SafetyBench,175,,,
2024,Multimodal,工具使用；多模态智能体,LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents,LLaVA-Plus：学习使用工具构建多模态智能体,216,https://arxiv.org/abs/2311.05437,https://github.com/LLaVA-VL/LLaVA-Plus-Codebase,761,,,
2024,Oral,视觉模型；基础模型,Sapiens: Foundation for Human Vision Models,Sapiens：面向人类视觉模型的基础模型,165,https://arxiv.org/pdf/2408.12569?,https://github.com/facebookresearch/sapiens?utm_source=chatgpt.com,5200,,,
2024,Multimodal,UI理解；多模态LLM,Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs,Ferret-UI：基于多模态大语言模型的可定位移动 UI 理解,161,https://arxiv.org/abs/2404.05719,https://github.com/apple/ml-ferret,8700,,,
2024,Oral+Multimodal,越狱；多模态模型,Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models,对齐的阿喀琉斯之踵：利用视觉脆弱性越狱多模态大语言模型,135,https://arxiv.org/pdf/2403.09792,https://github.com/RUCAIBox/HADES,32,,,
2024,Multimodal,视频理解；多模态智能体,VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding,VideoAgent：用于视频理解的记忆增强多模态智能体,129,https://arxiv.org/abs/2403.11481,https://github.com/YueFan1014/VideoAgent,272,,,
2024,Multimodal,遥感；多模态LLM,LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model,LHRS-Bot：结合志愿地理信息增强遥感多模态大语言模型,122,https://arxiv.org/abs/2402.02544,https://github.com/NJU-LHRS/LHRS-Bot,175,,,
2024,Multimodal,自动驾驶；多模态LLM,Dolphins: Multimodal Language Model for Driving,Dolphins：面向自动驾驶的多模态语言模型,121,https://arxiv.org/abs/2312.00438,,,,,
2024,Oral+Multimodal,信息检索；多模态,UniIR: Training and Benchmarking Universal Multimodal Information Retrievers,UniIR：通用多模态信息检索器的训练与评测,111,https://arxiv.org/pdf/2311.17136,https://github.com/TIGER-AI-Lab/UniIR?utm_source=chatgpt.com,168,,,
2024,Multimodal,视觉对话；多模态,LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models,LLaVA-Grounding：基于大型多模态模型的视觉定位对话,110,https://arxiv.org/abs/2312.02949,https://github.com/UX-Decoder/LLaVA-Grounding,419,,,
2024,Multimodal,视觉标注；Token化,Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models,Groma：用于多模态大语言模型定位的局部化视觉Token化方法,109,https://arxiv.org/abs/2404.13013,https://github.com/FoundationVision/Groma,580,,,
2024,Multimodal,安全防护；提示工程,AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting,AdaShield：通过自适应屏蔽提示防御结构化攻击的多模态大语言模型,107,https://arxiv.org/abs/2403.09513,https://github.com/rain305f/AdaShield,66,,,
2024,Multimodal,安全防护；图像转文本,"Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation","Eyes Closed, Safety On：通过图像到文本转换保护多模态大语言模型",100,https://arxiv.org/abs/2403.09572,,,,,
2024,Oral,文本渲染；扩散模型,TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering,TextDiffuser-2：释放语言模型在文本渲染中的能力,98,https://arxiv.org/pdf/2311.16465,https://github.com/microsoft/unilm/tree/master/textdiffuser-2?utm_source=chatgpt.com,21800,,,
2024,Multi-Modal,多模态评估；图像质量,Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models,超越分数的呈现：通过多模态语言模型推动图像质量评估,85,https://arxiv.org/abs/2312.08962,https://github.com/XPixelGroup/DepictQA,180,,,
2024,Medical,交互式分割；医学图像,ScribblePrompt: Fast and Flexible Interactive Segmentation for Any Biomedical Image,ScribblePrompt：面向任意生物医学图像的快速灵活交互式分割,77,https://arxiv.org/abs/2312.07381,https://github.com/halleewong/ScribblePrompt?utm_source=chatgpt.com,203,,,
2024,Multi-Modal,多模态生成；图像生成,MultiGen: Zero-shot Image Generation from Multi-modal Prompts,MultiGen：基于多模态提示的零样本图像生成,73,https://arxiv.org/abs/2403.14598,https://github.com/zamling/PSALM,262,,,
2024,Multi-Modal,多模态表征；地理空间,MMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial Representation Learning,MMEarth：探索用于地理空间表示学习的多模态预训练任务,69,https://arxiv.org/abs/2405.02771,https://github.com/vishalned/MMEarth-data,80,,,
2024,Oral+Multimodal,LLM；偏好优化,Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization,通过自举偏好优化增强多模态大语言模型,67,https://arxiv.org/pdf/2403.08730?,https://github.com/pipilurj/bootstrapped-preference-optimization-BPO?utm_source=chatgpt.com,59,,,
2024,Multimodal,图像质量；多模态评测,A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment,多模态大语言模型在图像质量评估上的系统研究,67,https://arxiv.org/abs/2403.10854,https://github.com/TianheWu/MLLMs-for-IQA,89,,,
2024,Oral,超分辨率；Transformer,HiT-SR: Hierarchical Transformer for Efficient Image Super-Resolution,HiT-SR：用于高效图像超分辨率的层级 Transformer,66,https://arxiv.org/pdf/2407.05878?,https://github.com/XiangZ-0/HiT-SR?utm_source=chatgpt.com,152,,,
2024,Multi-Modal,多模态 3D 检测；BEV,GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object Detection,GraphBEV：面向多模态三维目标检测的鲁棒鸟瞰图特征对齐,63,https://arxiv.org/abs/2403.11848,https://github.com/adept-thu/GraphBEV,120,,,
2024,Multimodal,多模态评测；视觉能力,BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models,BenchLMM：用于评测大型多模态模型跨风格视觉能力的基准,54,https://arxiv.org/abs/2312.02896,,,,,
2024,Multi-Modal,多模态动作；人体运动,Nymeria: A Massive Collection of Egocentric Multi-modal Human Motion in the Wild,Nymeria：大规模实景自视角多模态人体运动数据集,54,https://arxiv.org/abs/2406.09905,https://github.com/facebookresearch/nymeria_dataset,132,,,
2024,Multi-Modal,多模态分割；指代表达,SAM4MLLM: Enhance Multi-Modal Large Language Model for Referring Expression Segmentation,SAM4MLLM：增强多模态大模型以用于指代表达分割,53,https://arxiv.org/abs/2409.10542,https://github.com/AI-Application-and-Integration-Lab/SAM4MLLM,43,,,
2024,Medical,无监督配准；医学图像,Unsupervised Multi-modal Medical Image Registration via Invertible Translation,通过可逆翻译实现无监督多模态医学图像配准,51,https://arxiv.org/abs/2204.13656,https://github.com/heyblackC/DFMIR,51,,,
2024,Multimodal,音视频问答；多模态,CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios,CAT：增强多模态大语言模型在动态视听场景中的问答能力,49,https://arxiv.org/abs/2403.04640,https://github.com/rikeilong/Bay-CAT,57,,,
2024,Multimodal,前瞻推理；多模态,Merlin: Empowering Multimodal LLMs with Foresight Minds,Merlin：赋予多模态大语言模型前瞻性能力,47,https://arxiv.org/abs/2312.00589,https://github.com/Ahnsun/merlin,95,,,
2024,Foundation Model,位姿估计；零样本,FreeZe: Training-free zero-shot 6D pose estimation with geometric and vision foundation models,FreeZe：基于几何与视觉基础模型的免训练零样本6D位姿估计,43,https://arxiv.org/abs/2312.00947,https://github.com/andreacaraffa/freeze,99,,,
2024,Oral,AI 鉴别；检测,Zero-Shot Detection of AI-Generated Images,零样本 AI 生成图像检测,42,https://arxiv.org/pdf/2409.15875,https://github.com/grip-unina/ZED?utm_source=chatgpt.com,72,,,
2024,Medical,图学习；多模态表示,GTP-4o: Modality-prompted Heterogeneous Graph Learning for Omni-modal Biomedical Representation,GTP-4o：用于全模态生物医学表征的模态提示异构图学习,42,https://arxiv.org/abs/2407.05540,,,,,
2024,Oral+Multimodal,自监督；多模态,Decoupling Common and Unique Representations for Multimodal Self-supervised Learning,多模态自监督学习中共性与特有表示的解耦,41,https://arxiv.org/pdf/2309.05300,https://github.com/zhu-xlab/DeCUR?utm_source=chatgpt.com,70,,,
2024,Medical,半监督分割；教学策略,Alternate Diverse Teaching for Semi-supervised Medical Image Segmentation,半监督医学图像分割的交替多样化教学,40,https://arxiv.org/abs/2311.17325?utm_source=chatgpt.com,https://github.com/ZhenZHAO/AD-MT?utm_source=chatgpt.com,46,,,
2024,Multimodal,图像生成；模型适配,MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation,MoMA：用于快速个性化图像生成的多模态LLM适配器,40,https://arxiv.org/abs/2404.05674,https://github.com/bytedance/MoMA,234,,,
2024,Oral,视觉 Transformer；去噪,Denoising Vision Transformers,Denoising Vision Transformers：视觉 Transformer 去噪,39,https://arxiv.org/pdf/2401.02957,,,,,
2024,Multimodal,再学习；多模态平衡,Diagnosing and Re-learning for Balanced Multimodal Learning,用于平衡多模态学习的诊断与再学习,38,https://arxiv.org/abs/2407.09705,https://github.com/GeWu-Lab/Diagnosing_Relearning_ECCV2024,27,,,
2024,Cross-modal,图像超分辨；跨模态先验,XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution,XPSR：用于扩散式图像超分辨的跨模态先验,37,https://arxiv.org/abs/2403.05049,https://github.com/qyp2000/XPSR,76,,,
2024,Medical,医学图像分割；基础模型,I-MedSAM: Implicit Medical Image Segmentation with Segment Anything,I-MedSAM：基于 Segment Anything 的隐式医学图像分割,36,https://arxiv.org/abs/2311.17081?utm_source=chatgpt.com,https://github.com/ucwxb/I-MedSAM?utm_source=chatgpt.com,65,,,
2024,Medical,跨域公平性；医学图像,FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation and Classification,FairDomain：实现跨域医学图像分割与分类的公平性,33,https://arxiv.org/abs/2407.08813,https://github.com/Harvard-Ophthalmology-AI-Lab/FairDomain,36,,,
2024,Multimodal,情感分析；去偏,Towards Multimodal Sentiment Analysis Debiasing via Bias Purification,通过偏差消融实现多模态情感分析去偏,33,https://arxiv.org/abs/2403.05023,,,,,
2024,Oral,语义分割；多模态,Learning Modality-agnostic Representation for Semantic Segmentation from Any Modalities,从任意模态学习模态无关的语义分割表示,32,https://arxiv.org/pdf/2407.11351,,,,,
2024,Multimodal,脑解码；多模态,UMBRAE: Unified Multimodal Brain Decoding,UMBRAE：统一的多模态脑解码,32,https://arxiv.org/pdf/2404.07202?,https://github.com/weihaox/UMBRAE,56,,,
2024,Multimodal,多模态推理；数据集,HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning,HaloQuest：用于推进多模态推理的视觉幻觉数据集,31,https://arxiv.org/abs/2407.15680,https://github.com/ZhecanJamesWang/HaloQuest,5,,,
2024,Oral,规划模型；推理对齐,Making Large Language Models Better Planners with Reasoning-Decision Alignment,让大型语言模型成为更好的规划器：通过推理-决策对齐实现改进,30,https://arxiv.org/pdf/2408.13890?,,,,,
2024,Medical,多模态对比学习；医学图像,Improving Medical Multi-modal Contrastive Learning with Expert Annotations,利用专家标注改进医学多模态对比学习,29,https://arxiv.org/abs/2403.10153,https://github.com/ykumards/eCLIP?utm_source=chatgpt.com,9,,,
2024,Oral+Multimodal,病理；多模态基准,PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology,PathMMU：面向病理理解与推理的超大多模态专家级基准,24,https://arxiv.org/pdf/2401.16355,https://github.com/PathMMU-Benchmark/PathMMU?utm_source=chatgpt.com,27,,,
2024,Oral,SAM；小样本调优,CAT-SAM: Conditional Tuning for Few-Shot Adaptation of Segment Anything Model,CAT-SAM：Segment Anything 模型的小样本条件调优,24,https://arxiv.org/pdf/2402.03631,https://github.com/weihao1115/cat-sam?utm_source=chatgpt.com,135,,,
2024,Oral+Multimodal,多模态；生成助手,LLMGA: Multimodal Large Language Model based Generation Assistant,LLMGA：多模态大语言模型生成助手,23,https://arxiv.org/pdf/2311.16500,https://github.com/dvlab-research/LLMGA?utm_source=chatgpt.com,396,,,
2024,Multimodal,智能体；多模态基准,OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web,OmniACT：用于桌面与网页多模态通用智能体的基准数据集,23,https://arxiv.org/abs/2402.17553,,,23,,
2024,Multi-Modal,多模态定位；三维场景,WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language,WildRefer：基于多模态视觉与自然语言的大规模动态场景三维目标定位,23,https://arxiv.org/abs/2304.05645,https://github.com/4DVLab/WildRefer,7,,,
2024,Oral,病理；视觉语言,Knowledge-enhanced Visual-Language Pretraining for Computational Pathology,用于计算病理的知识增强视觉-语言预训练,22,https://arxiv.org/pdf/2404.09942,https://github.com/MAGIC-AI4Med/KEP?utm_source=chatgpt.com,45,,,
2024,Medical,模型鲁棒性；图像编辑,RadEdit: stress-testing biomedical vision models via diffusion image editing,RadEdit：基于扩散图像编辑对生物医学视觉模型进行压力测试,22,https://arxiv.org/abs/2312.12865?utm_source=chatgpt.com,https://huggingface.co/microsoft/radedit?utm_source=chatgpt.com,24,,,
2024,Multimodal,布局生成；检测Transformer,LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer,LayoutDETR：检测Transformer是优秀的多模态布局生成器,22,https://arxiv.org/abs/2212.09877,https://github.com/salesforce/LayoutDETR,100,,,
2024,Oral,视觉 Transformer；通用模型,GiT: Towards Generalist Vision Transformer through Universal Language Interface,GiT：通过通用语言接口迈向通用视觉 Transformer,20,https://arxiv.org/pdf/2403.09394,https://github.com/Haiyang-W/GiT?utm_source=chatgpt.com,356,,,
2024,Multi-Modal,多模态联邦；Transformer,Towards Multi-modal Transformers in Federated Learning,面向联邦学习的多模态Transformer,20,https://arxiv.org/abs/2404.12467,https://github.com/imguangyu/FedCola,19,,,
2024,Cross-modal,点云补全；跨模态,Explicitly Guided Information Interaction Network for Cross-modal Point Cloud Completion,显式引导的信息交互网络用于跨模态点云补全,19,https://arxiv.org/abs/2407.02887,https://github.com/WHU-USI3DV/EGIInet,55,,,
2024,Cross-modal,类别发现；跨模态教学,Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery,文本知识很重要：用于泛化视觉类别发现的跨模态协同教学,19,https://arxiv.org/abs/2403.07369,,,,,
2024,Multimodal,小样本；动作识别,Multimodal Cross-Domain Few-Shot Learning for Egocentric Action Recognition,面向第一视角动作识别的多模态跨域小样本学习,19,https://arxiv.org/abs/2405.19917,,,,,
2024,Multimodal,运动控制；多模态提示,MotionChain: Conversational Motion Controllers via Multimodal Prompts,MotionChain：基于多模态提示的对话式运动控制器,19,https://arxiv.org/abs/2404.01700,https://github.com/OpenMotionLab/MotionChain,68,,,
2024,Oral,数据增强；实例级,Dataset Enhancement with Instance-Level Augmentations,基于实例级增强的数据集增强方法,18,https://arxiv.org/pdf/2406.08249,https://github.com/KupynOrest/instance_augmentation?utm_source=chatgpt.com,44,,,
2024,Foundation Model,红外模型；基础模型,InfMAE: A Foundation Model in The Infrared Modality,InfMAE：用于红外模态的基础模型,18,https://arxiv.org/abs/2402.00407,,,,,
2024,Cross-modal,变化描述；对比学习,Distractors-Immune Representation Learning with Cross-modal Contrastive Regularization for Change Captioning,用于变化描述的具干扰免疫性的跨模态对比正则表示学习,17,https://arxiv.org/abs/2407.11683,https://github.com/tuyunbin/DIRL,11,,,
2024,Multimodal,多模态融合；3D检测,SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather,SAMFusion：用于恶劣天气中3D目标检测的传感器自适应多模态融合,17,https://arxiv.org/abs/2508.16408,,,,,
2024,Multimodal,域泛化；多模态,Towards Multimodal Open-Set Domain Generalization and Adaptation through Self-supervision,通过自监督实现多模态开放集域泛化与适应,17,https://arxiv.org/abs/2407.01518,https://github.com/donghao51/MOOSA,40,,,
2024,Multi-Modal,多模态检测；行人检测,When Pedestrian Detection Meets Multi-Modal Learning: Generalist Model and Benchmark Dataset,当行人检测遇上多模态学习：通用模型与基准数据集,17,https://arxiv.org/abs/2407.10125,https://github.com/BubblyYi/MMPedestron,100,,,
2024,Foundation Model,视觉跟踪；强化学习,Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL,利用视觉基础模型与离线强化学习增强具身视觉跟踪,16,https://arxiv.org/abs/2404.09857,https://github.com/UnrealTracking/Offline_RL_Active_Tracking,17,,,
2024,Foundation Model,视频压缩；无监督,Free-VSC: Free Semantics from Visual Foundation Models for Unsupervised Video Semantic Compression,Free-VSC：从视觉基础模型中获取自由语义的无监督视频语义压缩,16,https://arxiv.org/abs/2409.11718,,,,,
2024,Cross-modal,跨模态定位；场景图,SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs,SceneGraphLoc：基于3D场景图的跨模态粗粒度视觉定位,15,https://arxiv.org/abs/2404.00469,https://github.com/y9miao/VLSG,38,,,
2024,Cross-modal,跨模态行人再识别；域泛化,Domain Shifting: A Generalized Solution for Heterogeneous Cross-Modality Person Re-Identification,Domain Shifting：用于异构跨模态行人再识别的泛化解决方案,15,https://doi.org/10.1007/978-3-031-73220-1_17,https://github.com/Joey623/DNS,3,,,
2024,Cross-modal,视频去模糊；跨模态,CMTA: Cross-Modal Temporal Alignment for Event-guided Video Deblurring,CMTA：用于事件引导视频去模糊的跨模态时间对齐,15,https://arxiv.org/abs/2408.14930,https://github.com/intelpro/CMTA,14,,,
2024,Multimodal,模型遗忘；多模态,MultiDelete for Multimodal Machine Unlearning,MultiDelete：面向多模态机器遗忘的算法,15,https://arxiv.org/abs/2311.12047,https://github.com/CLU-UML/MultiDelete,8,,,
2024,Multimodal,数据生成；多模态LLM,GENIXER: Empowering Multimodal Large Language Models as a Powerful Data Generator,GENIXER：将多模态大语言模型赋能为强大的数据生成器,15,https://arxiv.org/abs/2312.06731,https://github.com/zhaohengyuan1/Genixer,114,,,
2024,Multimodal,农业诊断；多模态,A Multimodal Benchmark Dataset and Model for Crop Disease Diagnosis,用于农作物病害诊断的多模态基准与模型,15,https://arxiv.org/abs/2503.06973,https://github.com/UnicomAI/UnicomBenchmark/tree/main/CDDMBench,53,,,
2024,Multimodal,运动生成；多模态LLM,FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models,FreeMotion：基于多模态大语言模型的无动作捕捉人体运动生成,15,https://arxiv.org/abs/2406.10740,https://github.com/VankouF/FreeMotion-Codes,44,,,
2024,Oral,医学影像；重建,CardiacNet: Learning to Reconstruct Abnormalities for Cardiac Disease Assessment from Echocardiogram Videos,CardiacNet：通过异常重建学习心脏超声视频中的心脏疾病评估,14,https://arxiv.org/pdf/2410.20769,https://github.com/xmed-lab/CardiacNet?utm_source=chatgpt.com,21,,,
2024,Cross-modal,知识蒸馏；3D检测,LabelDistill: Label-guided Cross-modal Knowledge Distillation for Camera-based 3D Object Detection,LabelDistill：用于基于相机的3D目标检测的标签引导跨模态知识蒸馏,14,https://arxiv.org/abs/2407.10164,,,,,
2024,Multimodal,码本学习；多模态,UniCode : Learning a Unified Codebook for Multimodal Large Language Models,UniCode：用于多模态大语言模型的统一码本学习,14,https://arxiv.org/abs/2403.09072,,,,,
2024,Multimodal,模态补全；嵌入学习,Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding of Unimodal Models,通过单模态模型联合嵌入实现无配对多模态学习的缺失模态预测,14,https://arxiv.org/abs/2407.12616,,,,,
2024,Multimodal,交通监控；多模态基准,TrafficNight : An Aerial Multimodal Benchmark For Nighttime Vehicle Surveillance,TrafficNight：用于夜间车辆监测的多模态航拍基准,14,https://arxiv.org/abs/2407.07582,https://github.com/AIMSPolyU/TrafficNight,1,,,
2024,Multimodal,表格-图像；预训练,TIP: Tabular-Image Pre-training for Multimodal Classification with Incomplete Data,TIP：用于不完整数据的多模态分类的表格-图像预训练,14,https://arxiv.org/abs/2407.07582,https://github.com/siyi-wind/TIP,77,,,
2024,Foundation Model,立体匹配；域泛化,Learning Representations from Foundation Models for Domain Generalized Stereo Matching,从基础模型学习表征以实现域泛化立体匹配,14,https://arxiv.org/abs/2501.09898,,,,,
2024,Multimodal,小模型增强；多模态推理,Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training,通过自一致性训练提升小型多模态推理模型以匹敌大型模型,13,https://arxiv.org/abs/2311.14109,https://github.com/chengtan9907/mc-cot,24,,,
2024,Foundation Model,检索增强；数据增强,Enhancing Recipe Retrieval with Foundation Models: A Data Augmentation Perspective,利用基础模型增强菜谱检索：一种数据增强视角,13,https://arxiv.org/abs/2312.04763,https://github.com/Noah888/DAR,11,,,
2024,Medical,医学图像分割；自监督,AnatoMask: Enhancing Medical Image Segmentation with Reconstruction-guided Self-masking,AnatoMask：基于重建引导的自遮罩以提升医学图像分割,12,https://arxiv.org/pdf/2407.06468,https://github.com/ricklisz/AnatoMask?utm_source=chatgpt.com,35,,,
2024,Multimodal,鲁棒学习；表征解耦,Robust Multimodal Learning via Representation Decoupling,通过表征解耦实现鲁棒的多模态学习,12,https://arxiv.org/abs/2407.04458,https://github.com/shicaiwei123/ECCV2024-DMRNet,29,,,
2024,Multimodal,场景图；多模态,OpenPSG: Open-set Panoptic Scene Graph Generation via Large Multimodal Models,OpenPSG：基于大型多模态模型的开放集全景场景图生成,12,https://arxiv.org/abs/2407.11213,https://github.com/franciszzj/OpenPSG,49,,,
2024,Multimodal,视频理解；多模态序列,Learning Video Context as Interleaved Multimodal Sequences,将视频上下文学习为交织的多模态序列,12,https://arxiv.org/abs/2407.21757,https://github.com/showlab/MovieSeq,40,,,
2024,Multimodal,3D理解；多模态对齐,Dense Multimodal Alignment for Open-Vocabulary 3D Scene Understanding,用于开放词汇3D场景理解的稠密多模态对齐,12,https://arxiv.org/abs/2407.09781,https://github.com/lslrh/DMA,30,,,
2024,Medical,医学图像预训练；多模态,Unified Medical Image Pre-training in Language-Guided Common Semantic Space,在语言引导的通用语义空间中实现统一医学图像预训练,11,https://arxiv.org/abs/2311.14851,,,,,
2024,Foundation Model,目标检测；OOD,Can OOD Object Detectors Learn from Foundation Models?,OOD目标检测器能从基础模型中学习吗？,11,https://arxiv.org/abs/2409.05162,https://github.com/CVMI-Lab/SyncOOD,25,,,
2024,Oral,MRI；重建,Rethinking Deep Unrolled Model for Accelerated MRI Reconstruction,重新审视用于加速 MRI 重建的深度展开模型,10,https://drive.google.com/file/d/1urh--DH2OSO5KuOrwj2vUqDje_hxUhfk/view?pli=1,https://github.com/hellopipu/PromptMR-plus?utm_source=chatgpt.com,26,,,
2024,Oral+Foundation Models,部件分割；弱监督,WPS-SAM: Towards Weakly-Supervised Part Segmentation with Foundation Models,WPS-SAM：利用基础模型的弱监督部件分割,9,https://arxiv.org/pdf/2407.10131,https://github.com/xjwu1024/WPS-SAM?utm_source=chatgpt.com,12,,,
2024,Medical,半监督分割；类别不平衡,Gradient-Aware for Class-Imbalanced Semi-supervised Medical Image Segmentation,面向类别不平衡的半监督医学图像分割的梯度感知方法,9,https://link.springer.com/chapter/10.1007/978-3-031-73001-6_27,https://github.com/cicailalala/GALoss,8,,,
2024,Oral 最佳论文,视觉表示；极简建模,Minimalist Vision with Freeform Pixels,自由像素下的极简视觉,8,https://arxiv.org/pdf/2501.00142,https://github.com/ColumbiaComputerVision/mincam?utm_source=chatgpt.com,10,,,
2024,Cross-modal,跨模态对齐；多模态大模型,"X-InstructBLIP: A Framework for Aligning Image, 3D, Audio, Video to LLMs and its Emergent Cross-modal Reasoning",X-InstructBLIP：面向图像、3D、音频、视频与LLM对齐的框架及其跨模态推理能力,8,https://arxiv.org/abs/2311.18799,,,,,
2024,Multimodal,3D分割；多模态推理,PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model,PARIS3D：基于多模态大模型的推理式3D部件分割,8,https://arxiv.org/abs/2404.03836,https://github.com/AmrinKareem/PARIS3D,27,,,
2024,Multimodal,视觉补全；多模态LLM,Instruction Tuning-free Visual Token Complement for Multimodal LLMs,无需指令微调的多模态大语言模型视觉Token补全,8,https://arxiv.org/abs/2408.05019,,,,,
2024,Medical,半监督分割；泛化,The Devil is in the Statistics: Mitigating and Exploiting Statistics Difference for Generalizable Semi-supervised Medical Image Segmentation,关键在统计：缓解并利用统计差异以提升可泛化的半监督医学图像分割,7,https://arxiv.org/abs/2407.11356,https://github.com/qiumuyang/SIAB,11,,,
2024,Cross-modal,图文检索；跨模态,Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval,面向跨模态图文检索的目标感知查询扰动,7,https://arxiv.org/abs/2407.12346,https://github.com/NEC-N-SOGI/query-perturbation,8,,,
2024,Cross-modal,步态识别；跨模态,Camera-LiDAR Cross-modality Gait Recognition,相机–LiDAR跨模态步态识别,7,https://arxiv.org/abs/2407.02038,https://github.com/GWxuan/CL-Gait,7,,,
2024,Multi-Modal,多模态分割；像素级分割,PSALM: Pixelwise Segmentation with Large Multi-modal Model,PSALM：基于大型多模态模型的逐像素分割,7,https://arXiv.org/abs/2404.03144,,,,,
2024,Multi-Modal,多模态大模型；视觉混合,"SPHINX: A Mixer of Weights, Visual Embeddings and Image Scales for Multi-modal Large Language Models",SPHINX：用于多模态大模型的权重、视觉嵌入与图像尺度混合器,7,https://arxiv.org/abs/2311.07575,https://github.com/Alpha-VLLM/LLaMA2-Accessory,2800,,,
2024,Oral,Transformer；部件发现,PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers,PDiscoFormer：利用视觉 Transformer 放宽部件发现约束,6,https://arxiv.org/pdf/2407.04538,https://github.com/ananthu-aniraj/pdiscoformer?utm_source=chatgpt.com,17,,,
2024,Oral,图像识别；去偏差,From Fake to Real: Pretraining on Balanced Synthetic Images to Prevent Spurious Correlations in Image Recognition,从假到真：在平衡合成图像上预训练以防止图像识别中的伪相关,6,https://arxiv.org/pdf/2308.04553,https://github.com/mqraitem/From-Fake-to-Real?utm_source=chatgpt.com,2,,,
2024,Medical,报告生成；医学影像,MedRAT: Unpaired Medical Report Generation via Auxiliary Tasks,MedRAT：基于辅助任务的无配对医学报告生成,6,https://arxiv.org/abs/2407.03919,https://github.com/eladhi/MedRAT,3,,,
2024,Medical,医学图像配准；形变场,NePhi: Neural Deformation Fields for Approximately Diffeomorphic Medical Image Registration,NePhi：用于近似微分同胚医学图像配准的神经形变场,6,https://arxiv.org/abs/2309.07322,https://github.com/uncbiag/NePhi,8,,,
2024,Cross-modal,跨模态估计；自监督,SCPNet: Unsupervised Cross-modal Homography Estimation via Intra-modal Self-supervised Learning,SCPNet：通过模态内自监督学习的无监督跨模态单应估计,6,https://arxiv.org/abs/2407.08148,https://github.com/RM-Zhang/SCPNet,26,,,
2024,Multimodal,模型评测；指令冲突,Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions,检验张力：针对自相矛盾指令评测大型多模态模型,6,https://arXiv.org/abs/2408.01091,https://github.com/shiyegao/Self-Contradictory-Instructions-SCI,7,,,
2024,Multi-Modal,多模态融合；超分辨率,Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with Diffusion Model for Blind Image Super-Resolution,基于扩散模型的空间可变核自适应多模态融合盲超分辨,6,https://arxiv.org/abs/2403.05808,,,,,
2024,Oral,分割；小样本扩散,AlignDiff: Aligning Diffusion Models for General Few-Shot Segmentation,AlignDiff：面向通用小样本分割的扩散模型对齐,5,https://openreview.net/pdf?id=8nz6xYntfJ,https://github.com/RogerQi/AlignDiff?utm_source=chatgpt.com,7,,,
2024,Medical,半监督分割；医学图像,PMT: Progressive Mean Teacher via Exploring Temporal Consistency for Semi-Supervised Medical Image Segmentation,PMT：通过探索时间一致性的渐进式均值教师用于半监督医学图像分割,5,https://arxiv.org/abs/2409.05122,https://github.com/Axi404/PMT?utm_source=chatgpt.com,15,,,
2024,Cross-modal,视频定位；弱监督,Weakly-Supervised Spatio-Temporal Video Grounding with Variational Cross-Modal Alignment,弱监督时空视频定位的变分跨模态对齐,5,https://doi.org/10.1007/978-3-031-73195-2_24,,,,,
2024,Multimodal,图像检索；多模态提示,Elevating All Zero-Shot Sketch-Based Image Retrieval Through Multimodal Prompt Learning,通过多模态提示学习提升零样本素描图像检索,5,https://arxiv.org/abs/2407.04207,https://github.com/mainaksingha01/SpLIP,6,,,
2024,Multimodal,多模态对话；响应生成,BI-MDRG: Bridging Image History in Multimodal Dialogue Response Generation,BI-MDRG：多模态对话中桥接图像历史的响应生成,5,https://arxiv.org/abs/2408.05926,https://github.com/hee-suk-yoon/BI-MDRG,43,,,
2024,Foundation Model,失败检测；模型解释,DECIDER: Leveraging Foundation Model Priors for Improved Model Failure Detection and Explanation,DECIDER：利用基础模型先验改进模型失败检测与解释,5,https://arxiv.org/abs/2408.00331,https://github.com/kowshikthopalli/DECIDER/,0,,,
2024,Multi-Modal,多模态生成；扩散模型,MaxFusion: Plug&Play Multi-Modal Generation in Text-to-Image Diffusion Models,MaxFusion：文本到图像扩散模型中的即插即用多模态生成,5,https://arxiv.org/abs/2404.09977,https://github.com/Nithin-GK/MaxFusion,27,,,
2024,Foundation Model,注视预测；基础模型,Boosting Gaze Object Prediction via Pixel-level Supervision from Vision Foundation Model,通过视觉基础模型的像素级监督提升注视目标预测,4,https://arxiv.org/abs/2408.01044,https://github.com/jinyang06/SamGOP,7,,,
2024,Multi-Modal,多模态蒸馏；3D 表征,Multi-modal Relation Distillation for Unified 3D Representation Learning,用于统一三维表示学习的多模态关系蒸馏,4,https://arxiv.org/abs/2407.14007,,,,,
2024,Cross-modal,文本识别；对比学习,WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only Supervised Text Spotting,WeCromCL：面向仅转录监督文本识别的弱监督跨模态对比学习,3,https://arxiv.org/abs/2407.19507,,,,,
2024,Medical,液位估计；实验自动化,"Towards Dual Transparent Liquid Level Estimation in Biomedical Lab: Dataset, Methods and Practice",面向生物医学实验中双透明液位估计的数据集、方法与实践,2,https://link.springer.com/chapter/10.1007/978-3-031-73650-6_12,https://github.com/dualtransparency/TCLD,4,,,
2024,Multimodal,标签排序；多模态,Multimodal Label Relevance Ranking via Reinforcement Learning,通过强化学习的多模态标签相关性排序,2,https://arxiv.org/pdf/2407.13221?,https://github.com/ChazzyGordon/LR2PPO,4,,,
2024,Multi-Modal,姿态序列；多模态融合,Learning by Aligning 2D Skeleton Sequences and Multi-Modality Fusion,通过对齐二维骨架序列与多模态融合进行学习,2,https://arxiv.org/abs/2305.19480,,,,,
2024,Multi-Modal,多模态助手；3D 指令,M3DBench: Towards Omni 3D Assistant with Interleaved Multi-modal Instructions,M3DBench：面向全能三维助手的交错多模态指令基准,2,https://arxiv.org/abs/2312.10763,https://github.com/OpenM3D/M3DBench,61,,,
2024,Multimodal,模态对齐；多模态,Object-Oriented Anchoring and Modal Alignment in Multimodal Learning,面向多模态学习的面向对象锚定与模态对齐,1,https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06669.pdf,,,,,
2024,Oral,Transformer；样条,Spline-based Transformers,基于样条的 Transformer,0,https://arxiv.org/abs/2504.02797,https://github.com/lucidrains/spline-based-transformer/tree/main,105,,,
2024,Multi-Modal,医学多模态；MRI 融合,Energy-induced Explicit quantification for Multi-modality MRI fusion,用于多模态 MRI 融合的能量诱导显式量化,0,https://www.researchgate.net/profile/Shuo-Li-42/publication/382987950_Energy-induced_Explicit_quantification_for_Multi-modality_MRI_fusion/links/66b61cdd8f7e1236bc488220/Energy-induced-Explicit-quantification-for-Multi-modality-MRI-fusion.pdf,,,,,
